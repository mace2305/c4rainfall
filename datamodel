"""
what i want to do: 
- reduce redundencies (i.e. passing around configurations);
dependencies are passed around vis-a-vis a particular run index instead, maybe date, instead of a SET of configurations/reading folder names & matching. 

- simplify workflow - reduce granularity of control over functions, i.e. no need to initialize main() for all .py
presently there are 3 goals: 
(1) full train on DESIRED d/p/hp
(2) evaluation scoring for year-to-year dependencies (3y splits)
(3) *internal validation to domain, cluster_num, optimize hp, *yearly splits
(4) **setup.py, requirements.txt, Docker maybe?, data_load.py
"""

## present workflow: 

domain, period, raw data, variables required => prepare data -> flattened normalized datasets (of particular d, p, hp)
domain, period, hyperparam profile, flattened normalized datasets (of particular d, p, hp) => SOM train -> SOM model, SOM products
SOM products => visualize (SOM scatterplot-dmap) -> SOM plot
domain, period, hyperparam profile, SOM model => kmeans train -> kmeans scores, kmeans models
domain, period, hyperparam profile, kmeans model => derive cluster => visualize -> RHUM & Quiver & RF plots 
domain => generate domain permutations -> domains
for d in domains: repeat training for all, SOM models => internal validation for (2-25) clusters -> elbow/CH/DBI plot, ybrick plt, DBSCAN plt, sil plots
elbow/CH/DBI plot, ybrick plt, DBSCAN plt, sil plots => **HUMAN/MANUAL evaluation -> chosen cluster_num


## desired workflow:

(1) full-training for chosen d/p/hp into visualization (i.e. end-goal)
domain, period, raw data, variables required => prepare data -> input dataset, flattened normalized datasets (of particular d, p, hp)
hyperparam profile, flattened normalized datasets (of particular d, p, hp) => SOM train -> SOM model, SOM products
SOM products => visualize (SOM scatterplot-dmap) -> SOM plot
SOM model => kmeans train -> kmeans scores, kmeans models
input dataset, kmeans model => utils (subsetting by desired cluster_num) -> input dataset with cluster memberships
input dataset with cluster memberships => visualize -> RHUM & Quiver plots
input dataset with cluster memberships => utils (subsetting by date) -> input dataset from desired cluster
input dataset from desired cluster => visualize -> RF(avg) plots
input dataset from desired cluster => evaluate (using bootstrap resampling) -> CI for RF(avg)
CI for RF(avg) => visualize -> RF(lower-bound, upper-bound) plots


(2) evaluation
input dataset (of particular d,p), desired year "splits" => evaluate -> flattened normalized datasets (of particular d, p)
flattened normalized datasets (of particular d, p) => utils (subsetting by desired split) -> x_train, x_test, y_train, y_test
hyperparam profile, x_train => SOM train -> SOM model (for x_train)
SOM model (for x_train) => kmeans train -> kmeans score, kmeans models
kmeans models, x_train, y_train, y_test => utils (subsetting by desired cluster_num) => utils (subsetting by date) -> predicted RF arrays, "ground-truth" RF arrays
predicted RF arrays, "ground-truth" RF arrays => evaluate (using bootstrap resampling) -> avg(ROC, AUC, Brier scores) for this cluster 


(3) internal validation to determine appropriate domain (+ corresponding cluster_num), hp, & time-series/year splits
** still not figured out, but this is crucial if the library is to be dispensed

## desired workflow, higher-level (1024pm, 7Dec):

(*) hyperparameter optimization

(1) desired d/p/hp/cluster_num parameters, i.e. validation & parameter optimization
(1*) bash script to initiate (2a), with Luigi to note required prerequisite parameters/files
(1**) bash script calls python "DOMAIN generator" function, checks via Luigi if said parameters/outputs have been done, if so - SKIP! ==> this resolves memory issue by migrating to bash & using serial via Luigi/file-system checking
(1a) train SOM + kmeans (*need class since saving as .pkl with self.cluster_num will allow for use in (3b))
(1b) run validation metrices on clusters found
(1c) acquire best cluster_num + domain/hp combination for that period

(2) full train (with "Luigi"), based off desired d/p/hp/cluster_num parameters
(2a) run (1a) 
(2b) plot final output deliverables

(3) final evaluation of fully trained model(s)
(3*) bash script to initiate (3a), with Luigi to note required prerequisite parameters/files
- fully trained model (d/p/hp, cluster_num)
(3**) bash script calls python "d-p-hp-cluster_num SPLIT generator" function, checks via Luigi if said parameters/outputs have been done, if so - SKIP!
(3a) full train for x_train here, based off chosen split
(3b) acquire chosen cluster_num to subset y_train & y_test
(3c) acquire evaluation statistics for y_train via bootstrap resampling + compare to "actual" RF (*need to save weights, just in case)
(3d) summarize statistics across ALL splits

## desired Makefile (to shutdown/reopen kernel & prevent garbage issues)/.py (easier syntax) commands
- {with config having d/p/h} make validate4k
- {with config having d/p/h/cluster_num} make trainmodel (**kwargs, i.e. cluster_num = #)
- {with config having d/p/h/cluster_num/n_fold} make train_and_evaluate (**kwargs, i.e. cluster_num = #, n_fold = #)
    -> note: if model is already trained, it'll use that one, otherwise, is just automated training
- {with config having d/p/h} make multidomain_validate4k
- {with config having d/p/h/cluster_num} make multidomain_trainmodel (**kwargs, i.e. [[{domain}, cluster_num], [{domain}, cluster_num], ...])
- {with config having d/p/h/cluster_num/n_fold} make multidomain_train_and_evaluate (**kwargs, i.e. [[{domain}, cluster_num, n_fold], [{domain}, cluster_num, n_fold]])


### desired initialization file(s)
- function that runs (via Makefile or .py masterfile) which tracks changes in config.ini, or whatever settings.cfg/.ini and saves the old version by appending it to a params_history.txt



#### readjusted API (10Dec 142pm)
*. optimization of Hp (?)
0. generation of k & constraining of domain (D, P, Hp)
1. full model training & output visualization (D, P, Hp, k)
2. evaluation (incls. (1.), provide DELTA/no. bootstraps, PSI/year codependency, D, P, Hp, k)
3. evaluation w/o (1.), provide DELTA/no. bootstraps, PSI/year codependency, D, P, Hp, k) = note: you CANNOT evaluate without knowledge of k, unless (0.) is added into a new command (4. full-suite Hp optimization, k-generation, evaluation with DELTA & PSI, & full model output). 
4. X! needs (0.) to be built.

individual, INDEPENDENT modules: 0., 1., & 3. If *. is done, it will be INDEPENDENT as well. 

problem 2: how to split the above into files?


17 Dec funcs to links/attributes created
** types of scoring = CONDITIONS:
- > 1mm on grid = rain-day
==> boolean
- > 15mm on one day = heavy rain
==> boolean
- other random conditions (>=5mm, >=10mm, >=25mm...)

** scoring:
:: predict rain-day = T/F. True pos-s = correct prediction of grid outcome; vice versa. x_test is parsed through SOM -> kmeans -> 2nd-lvl cluster membership => via dates => y_pred (avg rainfall across grid for each cluster)
= basic simple accuracy for y_pred vs y_true at all conditions (1-5, 6-10, etc.), for all clusters, across the grid, for this d/p/hp/alpha split/'fold', for given CONDITION
= basic accuracy across all folds, across the grid for all d/p/hp, for given CONDITION
=> product: gridded outputs for a given CONDITION indicating accuracy scores

:: say for instance at SG is one lat/lon, that point through x_train has a proba of 0.7 to have >1mm for cluster 8 for xx d/p/hp. in the case of 0.7, that is ONE decision
boundary/"threshold". vary DBs from 0.1 to 1.0 => the TP vs. FP for each can be gotten!
= points on y_pred for each decision boundary, compare with y_true of that cluster => number TP/TN/FP/FN, for that cluster, for that decision boundary
= number TP/TN/FP/FN for all decision boundary, for that cluster
= AUC/ROC => TPR vs. FPR for all clusters, for that d/p/hp/alpha split ('fold')
= mean ROC for that d/p/hp (all splits/'folds') with AUC +- CI
:: sum up all TP/FP for each decision boundary across all splits & average = ROC curve
=> product: mean ROC curve with multiple ROC curves for each n-th fold, for a given CONDITION, "averaged by threshold" (https://people.inf.elte.hu/kiss/11dwhdm/roc.pdf)

:: x_train => clusters. y_train => RF for each cluster. x_test => separate to their clusters => y_pred is predicated RF for each cluster. y_test is the list of dates for each cluster, treated indepedently. However, instead of comparing y_pred (avg RF/cluster) vs y_test (list of G-T RFs for that cluster), take given CONDITION & compute proba for satisfying that CONDITION for y_train. Compare this proba to the proba of satisfying this condition for those in y_test => Brier scoring (means sq. error for proba predicted vs proba G-T), given a specific CONDITION.
= Brier score (proba_pred vs. proba_true) across the grid (green for close to 0, red for close to 1) for that cluster, for that decision boundary, for that d/p/hp/alpha split (i.e. "taking this set of PSI-years as Ground-truth, how good is this model at predicting probabilities of RF for each cluster, for the specified decision boundary?")
= avg Brier score across the grid for all clusters in that d/p/hp/alpha split, for that decision boundary
= avg Brier score across the grid for all clusters for all alpha splits in that d/p/hp, for that decision boundary (i.e. "how good is this model at predicting RF across the region specified (d)?")
= avg Brier scores across the grid for that cluster, for that decision boundary, for all alpha splits in that d/p/hp (i.e. "how good at predicting probability via this model (d/p/hp) for each cluster vs. another cluster")


tl_model = TopLevelModel(d,p,hp,domain_limits)
- self.LAT_S, self.LAT_N, self.LON_W, self.LON_E
- self.domain
- self.dir_str
- self.iterations, self.gridsize, self.training_mode, self.sigma, self.learning_rate, self.random_seed
- self.dir_hp_str
- self.period
- self.hyperparameters
- self.RUN_datetime
- self.RUN_time
- self.domain_limits
- self.domain_limits_str
- self.ALPHAs

alpha_model = AlphaLevelModel(tl_model) 
- self.alpha
- self.ALPHAs

AlphaLevelModel.check_evaluated(alpha)
- self.alpha_overall_dir (model_dir > alpha_overall_dir = find objs) 
- self.Brier_overall_path
- self.AUC_overall_path


TopLevelModel.detect_serialized_datasets(self)
- self.prepared_data_dir
*- self.raw_input_dir
*- self.raw_rf_dir
- self.input_ds_serialized_path
- self.rf_ds_serialized_path

TopLevelModel.detect_prepared_datasets(self)
- self.target_ds_preprocessed_path
- self.rf_ds_preprocessed_path
- self.standardized_stacked_arr_path

TopLevelModel.train_SOM(self)
- self.models_dir_path
- self.som_model_path

TopLevelModel.detect_som_products(self)
- self.winner_coordinates_path
- self.dmap_path
- self.ar_path
- self.som_weights_to_nodes_path

TopLevelModel.generate_k(self)
- self.metrics_dir_path
- self.ch_max_path
- self.dbi_min_path
- self.sil_peaks_path
- self.reasonable_sil_path
- self.ch_dbi_tally_path
- self.yellowbrick_expected_k_path
- self.dbs_err_dict_path
- self.n_expected_clusters_path

TopLevelModel.get_k(self)
- self.optimal_k
*- self.cluster_dir

TopLevelModel.train_kmeans(self)
- self.kmeans_model_path
*- self.labels_ar
*- self.labels_to_coords
*- self.label_markers
*- self.target_ds_withClusterLabels
*- self.dates_to_ClusterLabels
*- self.RFprec_to_ClusterLabels_dataset

TopLevelModel.serialize_kmeans_products(self)
- self.labels_ar
- self.labels_to_coords
- self.label_markers
- self.target_ds_withClusterLabels
- self.dates_to_ClusterLabels
- self.RFprec_to_ClusterLabels_dataset

TopLevelModel.print_outputs(self)
- NA

AlphaLevelModel.evaluation_procedure(alpha)
- 

AlphaLevelModel.compile_scores()
- 


*- self.
*- self.
*- self.
*- self.
*- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.
- self.



















































